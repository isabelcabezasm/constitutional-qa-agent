from typing import Literal, TypeVar

from agent_framework import ChatAgent
from anyio import Path
from pydantic import BaseModel

from core.qa_engine import QAEngine
from eval.metrics.models import (
    AccuracyEvaluationResults,
    EntityExtraction,
    TopicCoverageEvaluationResults,
)

T = TypeVar("T", bound=BaseModel)


class QAEvalEngine(QAEngine):
    """
    Question-Answering engine for evaluation of health insurance queries.

    This class extends QAEngine to provide specialized evaluation capabilities
    for accuracy assessment and entity extraction using structured outputs.

    Attributes:
        agent (ChatAgent): The chat agent for model inference (inherited).
        axiom_store (AxiomStore | None): Storage for axioms/constitution data
            (inherited).
    """

    _INPUT_VARIABLES = [
        "entity_list",
        "llm_answer",
        "user_query",
        "expected_answer",
        "expected_entities",
        "generated_entities",
    ]

    PromptTypes = Literal["accuracy", "entity_extraction", "system", "topic_coverage"]

    def __init__(
        self,
        agent: ChatAgent,
    ):
        """
        Initialize the QA Evaluation Engine.

        Args:
            chat: Azure OpenAI chat client instance for model inference.
        """
        # Initialize parent class without axiom_store since it's not needed
        # for evaluation
        super().__init__(agent, axiom_store=None)

    def _get_prompt(self, promptType: PromptTypes) -> str:
        """Load prompts."""

        file_path = Path(__file__).parent / "prompts" / f"{promptType}_prompt.md"
        with open(file_path, encoding="utf-8") as f:
            return f.read()

    async def _perform_model_invocation(self, prompt: str, output_type: type[T]) -> T:
        """Invoke the model and parse the output into the specified Pydantic model."""

        # Load system prompt
        system_prompt = self._get_prompt("system")

        # Use asyncio to run the async agent with structured output
        response = await self.agent.run(prompt, response_format=output_type)
        assert isinstance(response.value, output_type)
        return response.value

    async def entity_extraction(
        self, user_query: str, llm_answer: str, expected_answer: str
    ) -> EntityExtraction:
        """
        Extract and compare entities between the LLM-generated answer and expected
        answer.
        """
        metric_prompt = self._get_prompt("entity_extraction").format(
            user_query=user_query,
            llm_answer=llm_answer,
            expected_answer=expected_answer,
        )
        return await self._perform_model_invocation(metric_prompt, EntityExtraction)

    async def accuracy_evaluation(
        self, entity_list: EntityExtraction, llm_answer: str, expected_answer: str
    ) -> AccuracyEvaluationResults:
        """
        Evaluate the accuracy of question-answering responses for specific entities.

        This method assesses whether the underlying semantics and behaviors of the
        predicted entities in the LLM answer match those in the expected answer.

        Args:
            entity_list: List of entities to evaluate for accuracy.
            llm_answer: The answer generated by the language model.
            expected_answer: The ground truth answer for comparison.

        Returns:
            AccuracyEvaluation: The accuracy evaluation results for all entities.
        """
        # Convert entity list to a formatted string for the prompt
        entity_list_str = ", ".join(
            f"('{entity.trigger_variable}', '{entity.consequence_variable}')"
            for entity in entity_list.expected_answer_entities
        )

        metric_prompt = self._get_prompt("accuracy").format(
            entity_list=entity_list_str,
            llm_answer=llm_answer,
            expected_answer=expected_answer,
        )
        return await self._perform_model_invocation(
            metric_prompt, AccuracyEvaluationResults
        )

    async def topic_coverage_evaluation(
        self, entity_list: EntityExtraction
    ) -> TopicCoverageEvaluationResults:
        """
        Evaluate the topic coverage of the LLM-generated answer against the expected
        answer.

        This method assesses whether the topics represented by entities in the
        expected answer are covered in the generated answer. It focuses on recall
        (coverage) by checking if all expected entities appear in some form in the
        generated entities.
        """
        # Convert expected entities to a formatted string for the prompt
        expected_entities_str = ", ".join(
            f"('{entity.trigger_variable}', '{entity.consequence_variable}')"
            for entity in entity_list.expected_answer_entities
        )

        # Convert generated entities to a formatted string for the prompt
        generated_entities_str = ", ".join(
            f"('{entity.trigger_variable}', '{entity.consequence_variable}')"
            for entity in entity_list.llm_answer_entities
        )

        metric_prompt = self._get_prompt("topic_coverage").format(
            expected_entities=expected_entities_str,
            generated_entities=generated_entities_str,
        )
        return await self._perform_model_invocation(
            metric_prompt, TopicCoverageEvaluationResults
        )
