from typing import Literal, TypeVar

from agent_framework.azure import AzureOpenAIChatClient
from anyio import Path
from pydantic import BaseModel

from core.qa_engine import QAEngine
from eval.metrics.models import AccuracyEvaluationResults, EntityExtraction

T = TypeVar("T", bound=BaseModel)


class QAEvalEngine(QAEngine):
    """
    Question-Answering engine for evaluation of health insurance queries.

    This class extends QAEngine to provide specialized evaluation capabilities
    for accuracy assessment and entity extraction using structured outputs.

    Attributes:
        agent (ChatAgent): The chat agent for model inference (inherited).
        axiom_store (AxiomStore | None): Storage for axioms/constitution data
            (inherited).
    """

    _INPUT_VARIABLES = [
        "entity_list",
        "llm_answer",
        "user_query",
        "expected_answer",
        "expected_entities",
        "generated_entities",
    ]

    PromptTypes = Literal["accuracy", "entity_extraction", "system", "topic_coverage"]

    def __init__(
        self,
        chat: AzureOpenAIChatClient,
    ):
        """
        Initialize the QA Evaluation Engine.

        Args:
            chat: Azure OpenAI chat client instance for model inference.
        """
        # Initialize parent class without axiom_store since it's not needed
        # for evaluation
        super().__init__(chat, axiom_store=None)

    def _get_prompt(self, promptType: PromptTypes) -> str:
        """Load prompts."""

        file_path = Path(__file__).parent / "prompts" / f"{promptType}_prompt.md"
        with open(file_path, encoding="utf-8") as f:
            return f.read()

    async def _perform_model_invocation(self, prompt: str, output_type: type[T]) -> T:
        """Invoke the model and parse the output into the specified Pydantic model."""

        # Load system prompt
        system_prompt = self._get_prompt("system")

        # Create agent with system instructions if not already created
        # or if we need to update instructions (reuse parent's pattern)
        if self.agent is None:
            self.agent = self.chat.create_agent(instructions=system_prompt)

        # Use asyncio to run the async agent with structured output
        response = await self.agent.run(prompt, response_format=output_type)
        assert isinstance(response.value, output_type)
        return response.value

    async def entity_extraction(
        self, user_query: str, llm_answer: str, expected_answer: str
    ) -> EntityExtraction:
        """
        Extract and compare entities between the LLM-generated answer and expected
        answer.
        """
        metric_prompt = self._get_prompt("entity_extraction").format(
            user_query=user_query,
            llm_answer=llm_answer,
            expected_answer=expected_answer,
        )
        return await self._perform_model_invocation(metric_prompt, EntityExtraction)

    async def accuracy_evaluation(
        self, entity_list: EntityExtraction, llm_answer: str, expected_answer: str
    ) -> AccuracyEvaluationResults:
        """
        Evaluate the accuracy of question-answering responses for specific entities.

        This method assesses whether the underlying semantics and behaviors of the
        predicted entities in the LLM answer match those in the expected answer.

        Args:
            entity_list: List of entities to evaluate for accuracy.
            llm_answer: The answer generated by the language model.
            expected_answer: The ground truth answer for comparison.

        Returns:
            AccuracyEvaluation: The accuracy evaluation results for all entities.
        """
        # Convert entity list to a formatted string for the prompt
        entity_list_str = ", ".join(
            f"('{entity.trigger_variable}', '{entity.consequence_variable}')"
            for entity in entity_list.expected_answer_entities
        )

        metric_prompt = self._get_prompt("accuracy").format(
            entity_list=entity_list_str,
            llm_answer=llm_answer,
            expected_answer=expected_answer,
        )
        return await self._perform_model_invocation(
            metric_prompt, AccuracyEvaluationResults
        )

    async def topic_coverage_evaluator(self):
        """
        Evaluate the topic coverage of the LLM-generated answer against the expected
        answer.
        """
        pass
