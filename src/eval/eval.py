import asyncio
import json
from datetime import datetime
from pathlib import Path
from typing import Protocol

from pydantic import BaseModel

from core.paths import root
from eval.metrics.accuracy import get_accuracy
from eval.metrics.extract_entities import get_entities
from eval.metrics.models import (
    AccuracyEvaluationResults,
    EntityExtraction,
    TopicCoverageEvaluationResults,
)
from eval.metrics.topic_coverage import get_topic_coverage


class EvaluationSampleInput(BaseModel):
    """
    A data model representing input data for evaluation samples.

    This class defines the structure for evaluation sample inputs used in the evaluation
    process, containing all necessary information to assess model performance.

    Attributes:
        id (int): Unique identifier for the evaluation sample. query (str): The input
        question or prompt to be evaluated. context (str): Contextual information or
        background data relevant to the query. expected_answer (str): The correct or
        expected response for the given query. reasoning (list[str]): List of reasoning
        steps or explanations that lead to the expected answer. axioms_used (list[str]):
        List of axioms, rules, or principles applied in deriving the expected answer.
    """

    id: int
    query: str
    context: str
    expected_answer: str
    reasoning: list[str]
    axioms_used: list[str]


class EvaluationSampleOutput(BaseModel):
    """
    Represents the output of an evaluation sample containing input data, model response,
    and metrics.

    This class encapsulates the results of evaluating a single sample, including the
    original input, the language model's response, extracted entities, and computed
    performance metrics.

    Attributes:
        input (EvaluationSampleInput): The original input data used for evaluation
        llm_response (str): The response generated by the language model entities
        (list[str]): List of entities extracted from the response accuracy (float):
        Accuracy score for the evaluation sample (0.0 to 1.0) topic_coverage (float):
        Topic coverage score indicating how well the response
                               covers the expected topics (0.0 to 1.0)
    """

    input: EvaluationSampleInput
    llm_response: str
    entities: EntityExtraction
    accuracy: AccuracyEvaluationResults
    topic_coverage: TopicCoverageEvaluationResults


class Metric(BaseModel):
    """
    A data model representing statistical metrics with mean and standard deviation.

    Attributes:
        mean (float): The arithmetic mean of the data. std (float): The standard
        deviation of the data.
    """

    mean: float
    std: float


class AccuracyMetric(Metric):
    """
    A metric class for calculating accuracy of predictions.

    This metric computes the accuracy as the fraction of predictions that match the true
    labels. Accuracy is calculated as the number of correct predictions divided by the
    total number of predictions.

    The metric can be used for classification tasks where exact matches between
    predicted and actual values are required.

    Returns:
        float: Accuracy score between 0.0 and 1.0, where 1.0 represents perfect
        accuracy.
    """


class CoverageMetric(Metric):
    """
    A metric class for measuring topic coverage during evaluation.

    This metric tracks the degree to which responses cover expected topics or concepts
    in the evaluation samples. It provides insights into how comprehensively the
    system addresses the relevant subject matter.

    Attributes:
        Inherits all attributes from the base Metric class.

    Methods:
        Inherits all methods from the base Metric class and may override specific
        methods to implement coverage-specific calculations.

    Usage:
        Used to monitor and report topic coverage statistics during evaluation
        workflows.
    """


class QuestionAnswerFunction(Protocol):
    """
    Protocol for functions that generate answers from user queries.

    This protocol defines the interface for async functions that take a user's query
    string and return a generated answer. Implementations should process the query and
    generate contextually relevant responses based on the input.

    Methods:
        __call__(*, query: str) -> str: Async method that processes a query and
            returns a generated answer string.

    Example:
        >>> async def my_qa_function(*, query: str) -> str:
        ...     return f"Generated answer for: {query}"
        >>>
        >>> # Usage
        >>> answer = await my_qa_function(query="What if it rains?")
    """

    # this is just the protocol
    async def __call__(self, *, query: str) -> str:  # pyright: ignore[reportReturnType]
        """
        Takes a user query and returns a generated answer.

        Args:
            query: The user's query string

        Returns:
            A generated answer as a string
        """


class EvaluationResult(BaseModel):
    """
    Represents the complete result of an evaluation run.

    This class encapsulates all outputs and metrics from evaluating a model or system,
    providing a comprehensive view of performance across multiple dimensions.

    Attributes:
        evaluation_outputs (list[EvaluationSampleOutput]): A list of individual sample
            evaluation results, containing the detailed outputs for each test case.
        accuracy (AccuracyMetric): Metric measuring the correctness of predictions
            or responses across the evaluation dataset.
        topic_coverage (CoverageMetric): Metric measuring how well the evaluation
            spans different topics or categories in the domain.
    """

    evaluation_outputs: list[EvaluationSampleOutput]
    accuracy: AccuracyMetric
    topic_coverage: CoverageMetric


async def evaluate_answer(
    sample_input: EvaluationSampleInput, llm_answer: str
) -> EvaluationSampleOutput:
    """
    Evaluate the quality of an LLM's answer against the given input.

    This function assesses an LLM-generated answer by analyzing various metrics
    including accuracy and topic coverage, and extracting relevant entities.
    """

    entities = await get_entities(
        user_prompt=sample_input.query,
        llm_answer=llm_answer,
        expected_answer=sample_input.expected_answer,
    )

    accuracy = await get_accuracy(
        entity_list=entities,
        llm_answer=llm_answer,
        expected_answer=sample_input.expected_answer,
    )

    topic_coverage = await get_topic_coverage(entity_list=entities)

    return EvaluationSampleOutput(
        input=sample_input,
        llm_response=llm_answer,
        entities=entities,
        accuracy=accuracy,
        topic_coverage=topic_coverage,
    )


def calculate_stats(evaluation_results) -> EvaluationResult:
    """
    Calculate statistical metrics from evaluation results.
    
    Args:
        evaluation_results: Collection of evaluation outputs to analyze
        
    Returns:
        EvaluationResult: Object containing the evaluation outputs and computed
        statistical metrics including accuracy and topic coverage.
    """
    if not evaluation_results:
        return EvaluationResult(
            evaluation_outputs=[],
            accuracy=AccuracyMetric(mean=0.0, std=0.0),
            topic_coverage=CoverageMetric(mean=0.0, std=0.0),
        )

    # Calculate accuracy statistics
    accuracy_scores = [result.accuracy.accuracy_mean for result in evaluation_results]
    accuracy_mean = sum(accuracy_scores) / len(accuracy_scores)
    accuracy_variance = (
        sum((score - accuracy_mean) ** 2 for score in accuracy_scores) 
        / len(accuracy_scores)
    )
    accuracy_std = accuracy_variance ** 0.5 if len(accuracy_scores) > 1 else 0.0

    # Calculate topic coverage statistics
    coverage_scores = [
        result.topic_coverage.coverage_score for result in evaluation_results
    ]
    coverage_mean = sum(coverage_scores) / len(coverage_scores)
    coverage_variance = (
        sum((score - coverage_mean) ** 2 for score in coverage_scores) 
        / len(coverage_scores)
    )
    coverage_std = coverage_variance ** 0.5 if len(coverage_scores) > 1 else 0.0

    return EvaluationResult(
        evaluation_outputs=evaluation_results,
        accuracy=AccuracyMetric(mean=accuracy_mean, std=accuracy_std),
        topic_coverage=CoverageMetric(mean=coverage_mean, std=coverage_std),
    )


async def run_evaluation(
    *,
    question_answer_fn: QuestionAnswerFunction,
    input_data_path: Path | None = None,
    ouptput_data_path: Path | None = None,
) -> None:
    """
    Run the evaluation process with the given data path.

    Args:
        data_path (str): Path to the data file or directory
    """

    input_path = input_data_path or root() / "data/eval_dataset.json"
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_path = ouptput_data_path or root() / f"runs/{timestamp}"
    output_path.mkdir(parents=True, exist_ok=True)

    print(f"Running evaluation with data path: {input_path}")
    print(f"Running evaluation with output data path: {output_path}")

    async def process_sample(sample_data: str) -> EvaluationSampleOutput:
        parsed_input = EvaluationSampleInput.model_validate(sample_data)

        llm_response = await question_answer_fn(query=parsed_input.query)

        _ = (output_path / f"results_{parsed_input.id}.md").write_text(llm_response)
        return await evaluate_answer(parsed_input, llm_response)

    evaluation_results = await asyncio.gather(
        *map(process_sample, json.loads(input_path.read_text()))
    )
    result = calculate_stats(evaluation_results)

    # save the results
    result_path = output_path / "evaluation_results.json"
    _ = result_path.write_text(result.model_dump_json(indent=4))
