from core.dependencies import azure_chat_openai
from eval.llm_evaluator.qa_eval_engine import QAEvalEngine
from eval.metrics.models import AccuracyEvaluationResults, EntityExtraction


async def get_accuracy(
    *, entity_list: EntityExtraction, llm_answer: str, expected_answer: str
) -> AccuracyEvaluationResults:
    """
    Evaluate the accuracy of LLM predictions for specific entities using Azure OpenAI.

    This function assesses whether the underlying semantics and behaviors of the
    predicted entities in the LLM answer match those in the expected answer,
    regardless of how they are specifically expressed.

    Args:
        entity_list (list[str]): List of entities to evaluate for accuracy.
        llm_answer (str): The answer generated by the language model.
        expected_answer (str): The ground truth answer for comparison.

    Returns:
        AccuracyEvaluation: The accuracy evaluation results for all entities.
    """
    qa_eval_engine = QAEvalEngine(chat=azure_chat_openai())
    return await qa_eval_engine.accuracy_evaluation(
        entity_list, llm_answer, expected_answer
    )
